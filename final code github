{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4tJo-RoOkdn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random\n",
        "import time\n",
        "import difflib\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install -U scikit-fuzzy\n",
        "import skfuzzy as fuzz\n",
        "import skfuzzy.control as ctrl\n",
        "!pip install pyswarm\n",
        "import pyswarm as ps\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Upload the file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the Excel file scale 1 to 5\n",
        "file_name = '2000 Heart Care System copy gpt777.xlsx'\n",
        "df = pd.read_excel(file_name)\n",
        "\n",
        "required_columns = ['Requirement ID', 'Technical Dependency', 'User Preference']\n",
        "existing_columns = [str(col).strip() for col in df.columns]\n",
        "\n",
        "# Create a dictionary to map similar columns to required columns\n",
        "column_mapping = {}\n",
        "\n",
        "for col in required_columns:\n",
        "    # Find close matches for each required column\n",
        "    # Reduced cutoff to 0.6 for more flexibility in matching\n",
        "    close_match = difflib.get_close_matches(col, existing_columns, n=1, cutoff=0.6)\n",
        "\n",
        "    if not close_match:\n",
        "        raise KeyError(f\"Missing or similar column not found in the uploaded file: {col}\")\n",
        "    else:\n",
        "        print(f\"Found similar column: '{close_match[0]}' for required column: '{col}'\")\n",
        "        # Add the found match to the mapping dictionary\n",
        "        column_mapping[close_match[0]] = col\n",
        "\n",
        "# Rename the columns in the dataframe\n",
        "df = df.rename(columns=column_mapping)\n",
        "print(f\"Columns have been renamed as per required columns: {required_columns}\", end='\\n')\n",
        "\n",
        "# Normalize Technical Dependency and User Preference values\n",
        "scaler = MinMaxScaler()\n",
        "# Use the updated column names in case they were renamed\n",
        "df[['Technical Dependency', 'User Preference']] = scaler.fit_transform(df[['Technical Dependency', 'User Preference']])\n",
        "\n",
        "df_copy = df.copy()\n",
        "# Calculate the multiple combinations of final score using the follwoing weights\n",
        "def assign_priority(Technical_Dependency):\n",
        "    if Technical_Dependency >= 0.8:\n",
        "        return 1 #'High'\n",
        "    elif Technical_Dependency >= 0.5 and Technical_Dependency < 0.8:\n",
        "        return 2 #'Medium'\n",
        "    else:\n",
        "        return 3 #'Low'\n",
        "        # Function to calculate disagreement\n",
        "def calculate_disagreement(list1, list2):\n",
        "  list2 = list2[:len(list1)]\n",
        "  return sum(1 for a, b in zip(list1, list2) if a != b)\n",
        "\n",
        "# Fuzzy Logic prioritization function without Plots:\n",
        "def fuzzy_logic_prioritize_with_skfuzzy(requirements):\n",
        "  fuzzy_scores = {}\n",
        "\n",
        "  # Define universe variables & fuzzy membership ranges\n",
        "  x = np.array([0, 0.2, 0.6, 1])  # Breakpoints for fuzzy sets\n",
        "  low = fuzz.trapmf(x, [0, 0, 0.2, 0.4])   # Low priority\n",
        "  medium = fuzz.trapmf(x, [0.2, 0.4, 0.6, 0.8])  # Medium priority\n",
        "  high = fuzz.trapmf(x, [0.6, 0.8, 1.0, 1.0])   # High priority\n",
        "\n",
        "  for req in requirements:\n",
        "      score = df.loc[df['Requirement ID'] == req, 'Technical Dependency'].values[0]\n",
        "\n",
        "      # Compute fuzzy membership values\n",
        "      low_degree = fuzz.interp_membership(x, low, score)\n",
        "      medium_degree = fuzz.interp_membership(x, medium, score)\n",
        "      high_degree = fuzz.interp_membership(x, high, score)\n",
        "\n",
        "      # Assign a single priority value (weighted sum of membership values)\n",
        "      fuzzy_scores[req] = high_degree * 1 + medium_degree * 0.6 + low_degree * 0.2\n",
        "\n",
        "  # Sort requirements based on fuzzy scores in descending order\n",
        "  sorted_requirements = sorted(requirements, key=lambda x: fuzzy_scores[x], reverse=True)\n",
        "  return sorted_requirements\n",
        "\n",
        "# Tuned PSO prioritization function\n",
        "def pso_prioritize(requirements, pso_benchmark):\n",
        "    num_particles = 15\n",
        "    num_iterations = 15\n",
        "    w = 0.4  # tuned inertia\n",
        "    c1 = 1.5  # tuned cognitive\n",
        "    c2 = 2  # tuned social\n",
        "    bounds = ([-10, -10], [10, 10])\n",
        "    # Initialize particles as permutations of the requirements list\n",
        "    particles = [random.sample(requirements, len(requirements)) for _ in range(num_particles)]\n",
        "    velocities = [[random.uniform(-1, 1) for _ in range(len(requirements))] for _ in range(num_particles)]\n",
        "    personal_best_positions = particles[:]\n",
        "    global_best_position = min(particles, key=lambda p: calculate_disagreement(p, pso_benchmark))\n",
        "\n",
        "    # Iterate\n",
        "    for _ in range(num_iterations):\n",
        "        for i in range(num_particles):\n",
        "            # Update velocity and position\n",
        "            velocities[i] = [\n",
        "    max(-2, min(2, w * velocities[i][j] +\n",
        "        c1 * random.random() * (personal_best_positions[i][j] - particles[i][j]) +\n",
        "        c2 * random.random() * (global_best_position[j] - particles[i][j])))\n",
        "    for j in range(len(requirements))\n",
        "]\n",
        "\n",
        "            particles[i] = [\n",
        "                particles[i][j] + velocities[i][j]\n",
        "                for j in range(len(requirements))\n",
        "            ]\n",
        "\n",
        "            # Ensure particle values are valid indices in the requirements list\n",
        "\n",
        "            # Update particles to stay within valid range\n",
        "            particles[i] = [\n",
        "            requirements[max(0, min(len(requirements) - 1, int(round(value)))) % len(requirements)]\n",
        "            for value in particles[i]\n",
        "            ]\n",
        "\n",
        "            # Update personal and global best positions\n",
        "            if calculate_disagreement(particles[i], pso_benchmark[:len(particles[i])]) < calculate_disagreement(personal_best_positions[i], pso_benchmark[:len(personal_best_positions[i])]):\n",
        "                personal_best_positions[i] = particles[i][:]\n",
        "            if calculate_disagreement(particles[i], pso_benchmark[:len(particles[i])]) < calculate_disagreement(global_best_position, pso_benchmark[:len(global_best_position)]):\n",
        "                global_best_position = particles[i][:]\n",
        "            if calculate_disagreement(global_best_position, pso_benchmark[:len(global_best_position)]) < 1:\n",
        "             break\n",
        "\n",
        "    # Ensure global_best_position is sorted by the index in requirements\n",
        "    return sorted(global_best_position, key=lambda x: requirements.index(x))\n",
        "\n",
        "clusters = 3\n",
        "kmeans_clusters = {}\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "# Apply KMeans clustering into k groups based on normalized Technical Dependency and User Preference\n",
        "X = df[['Technical Dependency']]\n",
        "kmeans = KMeans(n_clusters=clusters, random_state=0, n_init='auto').fit(X)\n",
        "\n",
        "\n",
        "df[f'Labels_weight'] = kmeans.labels_\n",
        "\n",
        "print('Total High Req IDs {}:'.format(len(df[f'Labels_weight'].loc[df[f'Labels_weight'] == 1])))\n",
        "print('Total Medium Req IDs {}:'.format(len(df[f'Labels_weight'].loc[df[f'Labels_weight'] == 0])))\n",
        "print('Total Low Req IDs {}:'.format(len(df[f'Labels_weight'].loc[df[f'Labels_weight'] == 2])), end='\\n')\n",
        "\n",
        "# Making the Clusters:\n",
        "\n",
        "kmeans_clusters[f'Cluster High'] = df.loc[df[f'Labels_weight'] == 1]['Requirement ID'].tolist()\n",
        "kmeans_clusters[f'Cluster Medium'] = df.loc[df[f'Labels_weight'] == 0]['Requirement ID'].tolist()\n",
        "kmeans_clusters[f'Cluster Low'] = df.loc[df[f'Labels_weight'] == 2]['Requirement ID'].tolist()\n",
        "\n",
        "# Function to run the process for multiple runs\n",
        "def run_process(num_runs=10):\n",
        "  pso_accuracies = []\n",
        "  fuzzy_accuracies = []\n",
        "  final_solution_accuracies = []\n",
        "\n",
        "  pso_efficiencies = []\n",
        "  fuzzy_efficiencies = []\n",
        "  final_solution_efficiencies = []\n",
        "\n",
        "  # Define the number of runs\n",
        "  num_runs = 10  # You can change this value as needed\n",
        "  for _ in range(num_runs):\n",
        "    # Prioritize using PSO\n",
        "    start_time_pso_high = time.time()\n",
        "    pso_group_high = pso_prioritize(kmeans_clusters[f'Cluster High'], kmeans_clusters[f'Cluster High'])\n",
        "    end_time_pso_high = time.time()\n",
        "\n",
        "    start_time_pso_medium = time.time()\n",
        "    pso_group_medium = pso_prioritize(kmeans_clusters[f'Cluster Medium'], kmeans_clusters[f'Cluster Medium'])\n",
        "    end_time_pso_medium = time.time()\n",
        "\n",
        "    start_time_pso_low = time.time()\n",
        "    pso_group_low = pso_prioritize(kmeans_clusters[f'Cluster Low'], kmeans_clusters[f'Cluster Low'])\n",
        "    end_time_pso_low = time.time()\n",
        "\n",
        "    # Prioritize using Fuzzy Logic\n",
        "    start_time_fuzzy_high = time.time()\n",
        "    fuzzy_group_high = fuzzy_logic_prioritize_with_skfuzzy(kmeans_clusters[f'Cluster High'])\n",
        "    end_time_fuzzy_high = time.time()\n",
        "\n",
        "    start_time_fuzzy_medium = time.time()\n",
        "    fuzzy_group_medium = fuzzy_logic_prioritize_with_skfuzzy(kmeans_clusters[f'Cluster Medium'])\n",
        "    end_time_fuzzy_medium = time.time()\n",
        "\n",
        "    start_time_fuzzy_low = time.time()\n",
        "    fuzzy_group_low = fuzzy_logic_prioritize_with_skfuzzy(kmeans_clusters[f'Cluster Low'])\n",
        "    end_time_fuzzy_low = time.time()\n",
        "\n",
        "    disagreements = {\n",
        "        'pgroup_high': calculate_disagreement(pso_group_high, kmeans_clusters[f'Cluster High']),\n",
        "        'fgroup_high': calculate_disagreement(fuzzy_group_high, kmeans_clusters[f'Cluster High']),\n",
        "        'pgroup_medium': calculate_disagreement(pso_group_medium, kmeans_clusters[f'Cluster Medium']),\n",
        "        'fgroup_medium': calculate_disagreement(fuzzy_group_medium, kmeans_clusters[f'Cluster Medium']),\n",
        "        'pgroup_low': calculate_disagreement(pso_group_low, kmeans_clusters[f'Cluster Low']),\n",
        "        'fgroup_low': calculate_disagreement(fuzzy_group_low, kmeans_clusters[f'Cluster Low'])\n",
        "    }\n",
        "\n",
        "    total_length = len(kmeans_clusters[f'Cluster High']) + len(kmeans_clusters[f'Cluster Medium']) + len(kmeans_clusters[f'Cluster Low'])\n",
        "\n",
        "    # Individual Cluster Accuracies:\n",
        "    pso_accuracy_high = 1 - (disagreements['pgroup_high'] / total_length)\n",
        "    pso_accuracy_medium = 1 - (disagreements['pgroup_medium'] / total_length)\n",
        "    pso_accuracy_low = 1 - (disagreements['pgroup_low'] / total_length)\n",
        "    fuzzy_accuracy_high = 1 - (disagreements['fgroup_high'] / total_length)\n",
        "    fuzzy_accuracy_medium = 1 - (disagreements['fgroup_medium'] / total_length)\n",
        "    fuzzy_accuracy_low = 1 - (disagreements['fgroup_low'] / total_length)\n",
        "\n",
        "    # Final Solution\n",
        "    final_accuracy_high = pso_accuracy_high if pso_accuracy_high > fuzzy_accuracy_high else fuzzy_accuracy_high\n",
        "    final_accuracy_medium = pso_accuracy_medium if pso_accuracy_medium > fuzzy_accuracy_medium else fuzzy_accuracy_medium\n",
        "    final_accuracy_low = pso_accuracy_low if pso_accuracy_low > fuzzy_accuracy_low else fuzzy_accuracy_low\n",
        "\n",
        "    # Accuracies Mean:\n",
        "    pso_accuracy = (pso_accuracy_high + pso_accuracy_medium + pso_accuracy_low) / 3\n",
        "    fuzzy_accuracy = (fuzzy_accuracy_high + fuzzy_accuracy_medium + fuzzy_accuracy_low) / 3\n",
        "    final_solution_accuracy = (final_accuracy_high + final_accuracy_medium + final_accuracy_low) / 3\n",
        "\n",
        "    pso_efficiency_high = (end_time_pso_high - start_time_pso_high)\n",
        "    pso_efficiency_medium = (end_time_pso_medium - start_time_pso_medium)\n",
        "    pso_efficiency_low = (end_time_pso_low - start_time_pso_low)\n",
        "\n",
        "    fuzzy_efficiency_high = (end_time_fuzzy_high - start_time_fuzzy_high)\n",
        "    fuzzy_efficiency_medium = (end_time_fuzzy_medium - start_time_fuzzy_medium)\n",
        "    fuzzy_efficiency_low = (end_time_fuzzy_low - start_time_fuzzy_low)\n",
        "\n",
        "    final_time_high = min(pso_efficiency_high, fuzzy_efficiency_high)\n",
        "    final_time_medium = min(pso_efficiency_medium, fuzzy_efficiency_medium)\n",
        "    final_time_low = min(pso_efficiency_low, fuzzy_efficiency_low)\n",
        "\n",
        "    pso_efficiency = (pso_efficiency_high + pso_efficiency_medium + pso_efficiency_low) / 3\n",
        "    fuzzy_efficiency = (fuzzy_efficiency_high + fuzzy_efficiency_medium + fuzzy_efficiency_low)  / 3\n",
        "    final_solution_efficiency = (final_time_high + final_time_medium + final_time_low) / 3\n",
        "\n",
        "    pso_accuracies.append(pso_accuracy)\n",
        "    fuzzy_accuracies.append(fuzzy_accuracy)\n",
        "    final_solution_accuracies.append(final_solution_accuracy)\n",
        "\n",
        "    pso_efficiencies.append(pso_efficiency)\n",
        "    fuzzy_efficiencies.append(fuzzy_efficiency)\n",
        "    final_solution_efficiencies.append(final_solution_efficiency)\n",
        "\n",
        "  # Calculate averages\n",
        "  avg_pso_accuracy = np.mean(pso_accuracies)\n",
        "  avg_fuzzy_accuracy = np.mean(fuzzy_accuracies)\n",
        "  avg_final_solution_accuracy = np.mean(final_solution_accuracies)\n",
        "\n",
        "  avg_pso_efficiency = np.mean(pso_efficiencies)\n",
        "  avg_fuzzy_efficiency = np.mean(fuzzy_efficiencies)\n",
        "  avg_final_solution_efficiency = np.mean(final_solution_efficiencies)\n",
        "\n",
        "  print(f\"PSO Average Accuracy: {avg_pso_accuracy:.8f}\")\n",
        "  print(f\"Fuzzy Logic Average Accuracy: {avg_fuzzy_accuracy:.8f}\")\n",
        "  print(f\"Final Solution Average Accuracy: {avg_final_solution_accuracy:.8f}\")\n",
        "\n",
        "  print(f\"PSO Average Efficiency: {avg_pso_efficiency:.4f}\")\n",
        "  print(f\"Fuzzy Logic Average Efficiency: {avg_fuzzy_efficiency:.4f}\")\n",
        "  print(f\"Final Solution Average Efficiency: {avg_final_solution_efficiency:.4f}\", end='\\n')\n",
        "\n",
        "  # Plotting the results\n",
        "  accuracy_means = [avg_pso_accuracy, avg_fuzzy_accuracy, avg_final_solution_accuracy]\n",
        "  efficiency_means = [avg_pso_efficiency, avg_fuzzy_efficiency, avg_final_solution_efficiency]\n",
        "\n",
        "  labels = ['PSO', 'Fuzzy Logic', 'Final Solution']\n",
        "\n",
        "  x = np.arange(len(labels))\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 6))\n",
        "  width = 0.35  # the width of the bars\n",
        "\n",
        "  rects1 = ax.bar(x - width / 2, accuracy_means, width, label='Accuracy')\n",
        "  rects2 = ax.bar(x + width / 2, efficiency_means, width, label='Efficiency')\n",
        "\n",
        "  ax.set_ylabel('Scores')\n",
        "  ax.set_title('Average Accuracy and Efficiency by Method')\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(labels)\n",
        "  ax.legend()\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# Run the process for a specified number of runs (e.g., 10)\n",
        "run_process(num_runs=10)\n",
        "\n",
        "_ = fuzzy_logic_prioritize_with_skfuzzy_plots(kmeans_clusters[f'Cluster High'])\n",
        "_ = fuzzy_logic_prioritize_with_skfuzzy_plots(kmeans_clusters[f'Cluster Medium'])\n",
        "_ = fuzzy_logic_prioritize_with_skfuzzy_plots(kmeans_clusters[f'Cluster Low'])\n",
        "\n"
      ]
    }
  ]
}